{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k1ohwo3RTgk"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ArtIC-TITECH/b3-proj-2023/blob/main/docs/class_01.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbkbttwKRTgl"
      },
      "source": [
        "# ニューラルネットワークを用いたMNISTデータセットの学習\n",
        "\n",
        "## 目次\n",
        "\n",
        "1. ニューラルネットワークを用いた線形関数の近似\n",
        "2. 全結合層を使った手書き文字認識\n",
        "3. 畳み込み層を使った手書き文字認識"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ニューラルネットワークを用いた線形関数の近似"
      ],
      "metadata": {
        "id": "2uzqiE1ceEQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワークは、ChatGPTや画像生成AIなどの基盤となる技術です。  \n",
        "その基本原理は、関数近似に基づいており、複雑な関数を近似できるのが特徴です。  \n",
        "この章では、ニューラルネットワークで関数近似を行う簡単な例を紹介します。\n",
        "\n",
        "ニューラルネットワークの学習は以下の流れで行います。\n",
        "1. データセットの作成\n",
        "1. モデルの作成\n",
        "1. 損失関数、最適化関数の定義\n",
        "1. モデルの学習\n",
        "1. モデルの評価"
      ],
      "metadata": {
        "id": "3TzSjV2SeLps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "本題に入る前に本日使用するライブラリをimportします。"
      ],
      "metadata": {
        "id": "qgTb548WiIW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hcFX_ldXiHFE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1.データセットの作成"
      ],
      "metadata": {
        "id": "Q9GyIY_-e3FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワークの学習に用いるデータセットを作成します。今回の例ではy=2x+1の関数近似をします。"
      ],
      "metadata": {
        "id": "47NaEJFhix0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 2  # 傾き\n",
        "b = 1  # 切片\n",
        "x_train = np.linspace(0, 9, 10)  # 0から10までの100個のデータ点を生成\n",
        "y_train = a * x_train + b  # 真の線形関数"
      ],
      "metadata": {
        "id": "Qvb3Hr-weCGy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZghaA1njCzW",
        "outputId": "9d208072-fbcd-45cd-bfe8-22a5eaf34cac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oVGvaTajEmq",
        "outputId": "741f587d-f0a6-4db4-f82b-63556a50838e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19.])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "データ型をNumpyのarrayからPyTorchのtensorに変換します。PyTorchの処理の都合上、view(-1, 1)を使って形状を(10, 1)にします。"
      ],
      "metadata": {
        "id": "twJaS11vjsHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorに変換\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32).view(-1, 1)  # (10, 1) の形に\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # (10, 1) の形に"
      ],
      "metadata": {
        "id": "_lSIJdv2jscm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaBVAfh_kQcg",
        "outputId": "27dc9f3e-0ed8-4477-87d2-512ca8643bcf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [4.],\n",
              "        [5.],\n",
              "        [6.],\n",
              "        [7.],\n",
              "        [8.],\n",
              "        [9.]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W69ALx87kRqO",
        "outputId": "3d97bc96-08a6-4d97-b15f-6db3e084c646"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.],\n",
              "        [ 3.],\n",
              "        [ 5.],\n",
              "        [ 7.],\n",
              "        [ 9.],\n",
              "        [11.],\n",
              "        [13.],\n",
              "        [15.],\n",
              "        [17.],\n",
              "        [19.]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2.モデルの作成"
      ],
      "metadata": {
        "id": "Ucz-ckOGkdoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワークを作成します。  \n",
        "self.linear=nn.Linear(入力サイズ, 出力サイズ)は(出力サイズ$\\times$入力サイズ)の重み行列$\\mathbf{A}$と(出力サイズ)のバイアスベクトル$b$を生成します。  \n",
        "そして(入力サイズ)のベクトルxを入力した際、self.linear(x)は$\\mathbf{A}x+b$を返します。  \n",
        "今回は入力サイズも出力サイズも1なので$y=ax+b$とかけます。"
      ],
      "metadata": {
        "id": "QXtjOG9SlwBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # 入力1、出力1の線形層\n",
        "        init.constant_(self.linear.weight, 0)  # 重みaをすべて0に初期化\n",
        "        init.constant_(self.linear.bias, 0)    # バイアスbをすべて0に初期化\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# モデルのインスタンスを作成\n",
        "model = LinearRegressionModel()"
      ],
      "metadata": {
        "id": "IayH_46Ykqzy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "重みaとバイアスbをどちらも0にしたのでモデルはy=0x+0=0となり、入力に関わらず出力は0になります。"
      ],
      "metadata": {
        "id": "w490q3wxpZWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.Tensor([1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlNetiMHpFRQ",
        "outputId": "6075c2cf-e076-4e65-dd39-2c3ac985df46"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1-3. 損失関数, 最適化関数の定義"
      ],
      "metadata": {
        "id": "FA7Bdwwmq3-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "詳細は後述しますが  \n",
        "損失関数としてMSELoss (Mean squared error, 平均二乗誤差)を使用します。  \n",
        "また最適化関数としてSGD (Stochastic gradient descent, 確率的勾配降下法)を使用します。"
      ],
      "metadata": {
        "id": "vK7v5LGYrRv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()  # 平均二乗誤差を損失関数として使用\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01) # 確率的勾配降下法を最適化関数として使用"
      ],
      "metadata": {
        "id": "2C9khvF8q4VA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4. モデルの学習"
      ],
      "metadata": {
        "id": "TN_eR9GlsY4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "モデルの学習で重要なのが損失関数と最適化関数です。"
      ],
      "metadata": {
        "id": "l2Zj8os2vkoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 損失関数について\n",
        "平均二乗誤差を使ったlossはニューラルネットワークの出力y_pred=ax+bと正解データy_trainを用いて  \n",
        "$\\text{loss}=(\\text{y_pred}-\\text{y_train})^2=(ax+b-\\text{y_train})^2$  \n",
        "で計算されます。  \n",
        "例としてx=1のときy=2x+1の正解データはy_train=3です。しかしモデルの出力はy_pred=0だったとします。このときlossは$\\text{loss}=(0-3)^2=9$となります。モデルはlossができるだけ小さくなるように学習を行います。"
      ],
      "metadata": {
        "id": "R_0SsKxctOpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 最適化関数について\n",
        "lossを小さくするために、モデルは$y=ax+b$の$a$と$b$をそれぞれ大きくするべきか、小さくするべきか知りたいです。これを知るためにlossをaやbで微分した結果が役立ちます。  \n",
        "$$\n",
        "\\frac{\\partial\\text{loss}}{\\partial{a}}=2(ax+b-\\text{y_train})x\n",
        "$$\n",
        "が0より大きい場合、aを増やすとlossも大きくなります。反対に0より小さい場合、aを減らすとlossが大きくなります。  \n",
        "したがって$\\lambda(>0)$を微小量として以下のようにaを更新します。\n",
        "$$\n",
        "a = a-\\lambda\\frac{\\partial\\text{loss}}{\\partial{a}}\n",
        "$$\n",
        "$\\lambda$は最適化関数を定義する際にlrで指定します。  \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "k9DyAzj2vcBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 損失と勾配の確認"
      ],
      "metadata": {
        "id": "Q4gArbl5Ve6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際にy=0x+0にx=1を入れた際の損失と勾配を確認します。\n",
        "近似したい関数はy=2x+1だとします。"
      ],
      "metadata": {
        "id": "2pSDA8hMVlVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルのインスタンスを作成\n",
        "model = LinearRegressionModel()\n",
        "\n",
        "# x=1とします\n",
        "x = torch.Tensor([1])\n",
        "\n",
        "# モデルの予測\n",
        "y_pred = model(x)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "VacuWU1OVYxW",
        "outputId": "aa42f8eb-199d-47dd-b1f7-1e50121e02f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lossの表示\n",
        "loss = criterion(y_pred, 2*x+1)\n",
        "loss"
      ],
      "metadata": {
        "id": "IW5ZMT6SWexq",
        "outputId": "50dd9a7c-3ece-4f2e-dbab-dc03afd67f36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 勾配の計算\n",
        "loss.backward()\n",
        "\n",
        "# aの勾配の表示\n",
        "print(model.linear.weight.grad)"
      ],
      "metadata": {
        "id": "vpH6f8tEXDn7",
        "outputId": "6d0520c5-6cad-4981-8701-1c9988249c47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # モデルの予測\n",
        "    y_pred = model(x_train)\n",
        "\n",
        "    # 損失の計算\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # 勾配の初期化\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # バックプロパゲーション\n",
        "    loss.backward()\n",
        "\n",
        "    # 重みの更新\n",
        "    optimizer.step()\n",
        "\n",
        "    # 損失を表示\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21PH4hy7sfVP",
        "outputId": "c600fab6-e75d-4ced-8071-d67022ad676b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 133.0000\n",
            "Epoch [2/1000], Loss: 133.0000\n",
            "Epoch [3/1000], Loss: 133.0000\n",
            "Epoch [4/1000], Loss: 133.0000\n",
            "Epoch [5/1000], Loss: 133.0000\n",
            "Epoch [6/1000], Loss: 133.0000\n",
            "Epoch [7/1000], Loss: 133.0000\n",
            "Epoch [8/1000], Loss: 133.0000\n",
            "Epoch [9/1000], Loss: 133.0000\n",
            "Epoch [10/1000], Loss: 133.0000\n",
            "Epoch [11/1000], Loss: 133.0000\n",
            "Epoch [12/1000], Loss: 133.0000\n",
            "Epoch [13/1000], Loss: 133.0000\n",
            "Epoch [14/1000], Loss: 133.0000\n",
            "Epoch [15/1000], Loss: 133.0000\n",
            "Epoch [16/1000], Loss: 133.0000\n",
            "Epoch [17/1000], Loss: 133.0000\n",
            "Epoch [18/1000], Loss: 133.0000\n",
            "Epoch [19/1000], Loss: 133.0000\n",
            "Epoch [20/1000], Loss: 133.0000\n",
            "Epoch [21/1000], Loss: 133.0000\n",
            "Epoch [22/1000], Loss: 133.0000\n",
            "Epoch [23/1000], Loss: 133.0000\n",
            "Epoch [24/1000], Loss: 133.0000\n",
            "Epoch [25/1000], Loss: 133.0000\n",
            "Epoch [26/1000], Loss: 133.0000\n",
            "Epoch [27/1000], Loss: 133.0000\n",
            "Epoch [28/1000], Loss: 133.0000\n",
            "Epoch [29/1000], Loss: 133.0000\n",
            "Epoch [30/1000], Loss: 133.0000\n",
            "Epoch [31/1000], Loss: 133.0000\n",
            "Epoch [32/1000], Loss: 133.0000\n",
            "Epoch [33/1000], Loss: 133.0000\n",
            "Epoch [34/1000], Loss: 133.0000\n",
            "Epoch [35/1000], Loss: 133.0000\n",
            "Epoch [36/1000], Loss: 133.0000\n",
            "Epoch [37/1000], Loss: 133.0000\n",
            "Epoch [38/1000], Loss: 133.0000\n",
            "Epoch [39/1000], Loss: 133.0000\n",
            "Epoch [40/1000], Loss: 133.0000\n",
            "Epoch [41/1000], Loss: 133.0000\n",
            "Epoch [42/1000], Loss: 133.0000\n",
            "Epoch [43/1000], Loss: 133.0000\n",
            "Epoch [44/1000], Loss: 133.0000\n",
            "Epoch [45/1000], Loss: 133.0000\n",
            "Epoch [46/1000], Loss: 133.0000\n",
            "Epoch [47/1000], Loss: 133.0000\n",
            "Epoch [48/1000], Loss: 133.0000\n",
            "Epoch [49/1000], Loss: 133.0000\n",
            "Epoch [50/1000], Loss: 133.0000\n",
            "Epoch [51/1000], Loss: 133.0000\n",
            "Epoch [52/1000], Loss: 133.0000\n",
            "Epoch [53/1000], Loss: 133.0000\n",
            "Epoch [54/1000], Loss: 133.0000\n",
            "Epoch [55/1000], Loss: 133.0000\n",
            "Epoch [56/1000], Loss: 133.0000\n",
            "Epoch [57/1000], Loss: 133.0000\n",
            "Epoch [58/1000], Loss: 133.0000\n",
            "Epoch [59/1000], Loss: 133.0000\n",
            "Epoch [60/1000], Loss: 133.0000\n",
            "Epoch [61/1000], Loss: 133.0000\n",
            "Epoch [62/1000], Loss: 133.0000\n",
            "Epoch [63/1000], Loss: 133.0000\n",
            "Epoch [64/1000], Loss: 133.0000\n",
            "Epoch [65/1000], Loss: 133.0000\n",
            "Epoch [66/1000], Loss: 133.0000\n",
            "Epoch [67/1000], Loss: 133.0000\n",
            "Epoch [68/1000], Loss: 133.0000\n",
            "Epoch [69/1000], Loss: 133.0000\n",
            "Epoch [70/1000], Loss: 133.0000\n",
            "Epoch [71/1000], Loss: 133.0000\n",
            "Epoch [72/1000], Loss: 133.0000\n",
            "Epoch [73/1000], Loss: 133.0000\n",
            "Epoch [74/1000], Loss: 133.0000\n",
            "Epoch [75/1000], Loss: 133.0000\n",
            "Epoch [76/1000], Loss: 133.0000\n",
            "Epoch [77/1000], Loss: 133.0000\n",
            "Epoch [78/1000], Loss: 133.0000\n",
            "Epoch [79/1000], Loss: 133.0000\n",
            "Epoch [80/1000], Loss: 133.0000\n",
            "Epoch [81/1000], Loss: 133.0000\n",
            "Epoch [82/1000], Loss: 133.0000\n",
            "Epoch [83/1000], Loss: 133.0000\n",
            "Epoch [84/1000], Loss: 133.0000\n",
            "Epoch [85/1000], Loss: 133.0000\n",
            "Epoch [86/1000], Loss: 133.0000\n",
            "Epoch [87/1000], Loss: 133.0000\n",
            "Epoch [88/1000], Loss: 133.0000\n",
            "Epoch [89/1000], Loss: 133.0000\n",
            "Epoch [90/1000], Loss: 133.0000\n",
            "Epoch [91/1000], Loss: 133.0000\n",
            "Epoch [92/1000], Loss: 133.0000\n",
            "Epoch [93/1000], Loss: 133.0000\n",
            "Epoch [94/1000], Loss: 133.0000\n",
            "Epoch [95/1000], Loss: 133.0000\n",
            "Epoch [96/1000], Loss: 133.0000\n",
            "Epoch [97/1000], Loss: 133.0000\n",
            "Epoch [98/1000], Loss: 133.0000\n",
            "Epoch [99/1000], Loss: 133.0000\n",
            "Epoch [100/1000], Loss: 133.0000\n",
            "Epoch [101/1000], Loss: 133.0000\n",
            "Epoch [102/1000], Loss: 133.0000\n",
            "Epoch [103/1000], Loss: 133.0000\n",
            "Epoch [104/1000], Loss: 133.0000\n",
            "Epoch [105/1000], Loss: 133.0000\n",
            "Epoch [106/1000], Loss: 133.0000\n",
            "Epoch [107/1000], Loss: 133.0000\n",
            "Epoch [108/1000], Loss: 133.0000\n",
            "Epoch [109/1000], Loss: 133.0000\n",
            "Epoch [110/1000], Loss: 133.0000\n",
            "Epoch [111/1000], Loss: 133.0000\n",
            "Epoch [112/1000], Loss: 133.0000\n",
            "Epoch [113/1000], Loss: 133.0000\n",
            "Epoch [114/1000], Loss: 133.0000\n",
            "Epoch [115/1000], Loss: 133.0000\n",
            "Epoch [116/1000], Loss: 133.0000\n",
            "Epoch [117/1000], Loss: 133.0000\n",
            "Epoch [118/1000], Loss: 133.0000\n",
            "Epoch [119/1000], Loss: 133.0000\n",
            "Epoch [120/1000], Loss: 133.0000\n",
            "Epoch [121/1000], Loss: 133.0000\n",
            "Epoch [122/1000], Loss: 133.0000\n",
            "Epoch [123/1000], Loss: 133.0000\n",
            "Epoch [124/1000], Loss: 133.0000\n",
            "Epoch [125/1000], Loss: 133.0000\n",
            "Epoch [126/1000], Loss: 133.0000\n",
            "Epoch [127/1000], Loss: 133.0000\n",
            "Epoch [128/1000], Loss: 133.0000\n",
            "Epoch [129/1000], Loss: 133.0000\n",
            "Epoch [130/1000], Loss: 133.0000\n",
            "Epoch [131/1000], Loss: 133.0000\n",
            "Epoch [132/1000], Loss: 133.0000\n",
            "Epoch [133/1000], Loss: 133.0000\n",
            "Epoch [134/1000], Loss: 133.0000\n",
            "Epoch [135/1000], Loss: 133.0000\n",
            "Epoch [136/1000], Loss: 133.0000\n",
            "Epoch [137/1000], Loss: 133.0000\n",
            "Epoch [138/1000], Loss: 133.0000\n",
            "Epoch [139/1000], Loss: 133.0000\n",
            "Epoch [140/1000], Loss: 133.0000\n",
            "Epoch [141/1000], Loss: 133.0000\n",
            "Epoch [142/1000], Loss: 133.0000\n",
            "Epoch [143/1000], Loss: 133.0000\n",
            "Epoch [144/1000], Loss: 133.0000\n",
            "Epoch [145/1000], Loss: 133.0000\n",
            "Epoch [146/1000], Loss: 133.0000\n",
            "Epoch [147/1000], Loss: 133.0000\n",
            "Epoch [148/1000], Loss: 133.0000\n",
            "Epoch [149/1000], Loss: 133.0000\n",
            "Epoch [150/1000], Loss: 133.0000\n",
            "Epoch [151/1000], Loss: 133.0000\n",
            "Epoch [152/1000], Loss: 133.0000\n",
            "Epoch [153/1000], Loss: 133.0000\n",
            "Epoch [154/1000], Loss: 133.0000\n",
            "Epoch [155/1000], Loss: 133.0000\n",
            "Epoch [156/1000], Loss: 133.0000\n",
            "Epoch [157/1000], Loss: 133.0000\n",
            "Epoch [158/1000], Loss: 133.0000\n",
            "Epoch [159/1000], Loss: 133.0000\n",
            "Epoch [160/1000], Loss: 133.0000\n",
            "Epoch [161/1000], Loss: 133.0000\n",
            "Epoch [162/1000], Loss: 133.0000\n",
            "Epoch [163/1000], Loss: 133.0000\n",
            "Epoch [164/1000], Loss: 133.0000\n",
            "Epoch [165/1000], Loss: 133.0000\n",
            "Epoch [166/1000], Loss: 133.0000\n",
            "Epoch [167/1000], Loss: 133.0000\n",
            "Epoch [168/1000], Loss: 133.0000\n",
            "Epoch [169/1000], Loss: 133.0000\n",
            "Epoch [170/1000], Loss: 133.0000\n",
            "Epoch [171/1000], Loss: 133.0000\n",
            "Epoch [172/1000], Loss: 133.0000\n",
            "Epoch [173/1000], Loss: 133.0000\n",
            "Epoch [174/1000], Loss: 133.0000\n",
            "Epoch [175/1000], Loss: 133.0000\n",
            "Epoch [176/1000], Loss: 133.0000\n",
            "Epoch [177/1000], Loss: 133.0000\n",
            "Epoch [178/1000], Loss: 133.0000\n",
            "Epoch [179/1000], Loss: 133.0000\n",
            "Epoch [180/1000], Loss: 133.0000\n",
            "Epoch [181/1000], Loss: 133.0000\n",
            "Epoch [182/1000], Loss: 133.0000\n",
            "Epoch [183/1000], Loss: 133.0000\n",
            "Epoch [184/1000], Loss: 133.0000\n",
            "Epoch [185/1000], Loss: 133.0000\n",
            "Epoch [186/1000], Loss: 133.0000\n",
            "Epoch [187/1000], Loss: 133.0000\n",
            "Epoch [188/1000], Loss: 133.0000\n",
            "Epoch [189/1000], Loss: 133.0000\n",
            "Epoch [190/1000], Loss: 133.0000\n",
            "Epoch [191/1000], Loss: 133.0000\n",
            "Epoch [192/1000], Loss: 133.0000\n",
            "Epoch [193/1000], Loss: 133.0000\n",
            "Epoch [194/1000], Loss: 133.0000\n",
            "Epoch [195/1000], Loss: 133.0000\n",
            "Epoch [196/1000], Loss: 133.0000\n",
            "Epoch [197/1000], Loss: 133.0000\n",
            "Epoch [198/1000], Loss: 133.0000\n",
            "Epoch [199/1000], Loss: 133.0000\n",
            "Epoch [200/1000], Loss: 133.0000\n",
            "Epoch [201/1000], Loss: 133.0000\n",
            "Epoch [202/1000], Loss: 133.0000\n",
            "Epoch [203/1000], Loss: 133.0000\n",
            "Epoch [204/1000], Loss: 133.0000\n",
            "Epoch [205/1000], Loss: 133.0000\n",
            "Epoch [206/1000], Loss: 133.0000\n",
            "Epoch [207/1000], Loss: 133.0000\n",
            "Epoch [208/1000], Loss: 133.0000\n",
            "Epoch [209/1000], Loss: 133.0000\n",
            "Epoch [210/1000], Loss: 133.0000\n",
            "Epoch [211/1000], Loss: 133.0000\n",
            "Epoch [212/1000], Loss: 133.0000\n",
            "Epoch [213/1000], Loss: 133.0000\n",
            "Epoch [214/1000], Loss: 133.0000\n",
            "Epoch [215/1000], Loss: 133.0000\n",
            "Epoch [216/1000], Loss: 133.0000\n",
            "Epoch [217/1000], Loss: 133.0000\n",
            "Epoch [218/1000], Loss: 133.0000\n",
            "Epoch [219/1000], Loss: 133.0000\n",
            "Epoch [220/1000], Loss: 133.0000\n",
            "Epoch [221/1000], Loss: 133.0000\n",
            "Epoch [222/1000], Loss: 133.0000\n",
            "Epoch [223/1000], Loss: 133.0000\n",
            "Epoch [224/1000], Loss: 133.0000\n",
            "Epoch [225/1000], Loss: 133.0000\n",
            "Epoch [226/1000], Loss: 133.0000\n",
            "Epoch [227/1000], Loss: 133.0000\n",
            "Epoch [228/1000], Loss: 133.0000\n",
            "Epoch [229/1000], Loss: 133.0000\n",
            "Epoch [230/1000], Loss: 133.0000\n",
            "Epoch [231/1000], Loss: 133.0000\n",
            "Epoch [232/1000], Loss: 133.0000\n",
            "Epoch [233/1000], Loss: 133.0000\n",
            "Epoch [234/1000], Loss: 133.0000\n",
            "Epoch [235/1000], Loss: 133.0000\n",
            "Epoch [236/1000], Loss: 133.0000\n",
            "Epoch [237/1000], Loss: 133.0000\n",
            "Epoch [238/1000], Loss: 133.0000\n",
            "Epoch [239/1000], Loss: 133.0000\n",
            "Epoch [240/1000], Loss: 133.0000\n",
            "Epoch [241/1000], Loss: 133.0000\n",
            "Epoch [242/1000], Loss: 133.0000\n",
            "Epoch [243/1000], Loss: 133.0000\n",
            "Epoch [244/1000], Loss: 133.0000\n",
            "Epoch [245/1000], Loss: 133.0000\n",
            "Epoch [246/1000], Loss: 133.0000\n",
            "Epoch [247/1000], Loss: 133.0000\n",
            "Epoch [248/1000], Loss: 133.0000\n",
            "Epoch [249/1000], Loss: 133.0000\n",
            "Epoch [250/1000], Loss: 133.0000\n",
            "Epoch [251/1000], Loss: 133.0000\n",
            "Epoch [252/1000], Loss: 133.0000\n",
            "Epoch [253/1000], Loss: 133.0000\n",
            "Epoch [254/1000], Loss: 133.0000\n",
            "Epoch [255/1000], Loss: 133.0000\n",
            "Epoch [256/1000], Loss: 133.0000\n",
            "Epoch [257/1000], Loss: 133.0000\n",
            "Epoch [258/1000], Loss: 133.0000\n",
            "Epoch [259/1000], Loss: 133.0000\n",
            "Epoch [260/1000], Loss: 133.0000\n",
            "Epoch [261/1000], Loss: 133.0000\n",
            "Epoch [262/1000], Loss: 133.0000\n",
            "Epoch [263/1000], Loss: 133.0000\n",
            "Epoch [264/1000], Loss: 133.0000\n",
            "Epoch [265/1000], Loss: 133.0000\n",
            "Epoch [266/1000], Loss: 133.0000\n",
            "Epoch [267/1000], Loss: 133.0000\n",
            "Epoch [268/1000], Loss: 133.0000\n",
            "Epoch [269/1000], Loss: 133.0000\n",
            "Epoch [270/1000], Loss: 133.0000\n",
            "Epoch [271/1000], Loss: 133.0000\n",
            "Epoch [272/1000], Loss: 133.0000\n",
            "Epoch [273/1000], Loss: 133.0000\n",
            "Epoch [274/1000], Loss: 133.0000\n",
            "Epoch [275/1000], Loss: 133.0000\n",
            "Epoch [276/1000], Loss: 133.0000\n",
            "Epoch [277/1000], Loss: 133.0000\n",
            "Epoch [278/1000], Loss: 133.0000\n",
            "Epoch [279/1000], Loss: 133.0000\n",
            "Epoch [280/1000], Loss: 133.0000\n",
            "Epoch [281/1000], Loss: 133.0000\n",
            "Epoch [282/1000], Loss: 133.0000\n",
            "Epoch [283/1000], Loss: 133.0000\n",
            "Epoch [284/1000], Loss: 133.0000\n",
            "Epoch [285/1000], Loss: 133.0000\n",
            "Epoch [286/1000], Loss: 133.0000\n",
            "Epoch [287/1000], Loss: 133.0000\n",
            "Epoch [288/1000], Loss: 133.0000\n",
            "Epoch [289/1000], Loss: 133.0000\n",
            "Epoch [290/1000], Loss: 133.0000\n",
            "Epoch [291/1000], Loss: 133.0000\n",
            "Epoch [292/1000], Loss: 133.0000\n",
            "Epoch [293/1000], Loss: 133.0000\n",
            "Epoch [294/1000], Loss: 133.0000\n",
            "Epoch [295/1000], Loss: 133.0000\n",
            "Epoch [296/1000], Loss: 133.0000\n",
            "Epoch [297/1000], Loss: 133.0000\n",
            "Epoch [298/1000], Loss: 133.0000\n",
            "Epoch [299/1000], Loss: 133.0000\n",
            "Epoch [300/1000], Loss: 133.0000\n",
            "Epoch [301/1000], Loss: 133.0000\n",
            "Epoch [302/1000], Loss: 133.0000\n",
            "Epoch [303/1000], Loss: 133.0000\n",
            "Epoch [304/1000], Loss: 133.0000\n",
            "Epoch [305/1000], Loss: 133.0000\n",
            "Epoch [306/1000], Loss: 133.0000\n",
            "Epoch [307/1000], Loss: 133.0000\n",
            "Epoch [308/1000], Loss: 133.0000\n",
            "Epoch [309/1000], Loss: 133.0000\n",
            "Epoch [310/1000], Loss: 133.0000\n",
            "Epoch [311/1000], Loss: 133.0000\n",
            "Epoch [312/1000], Loss: 133.0000\n",
            "Epoch [313/1000], Loss: 133.0000\n",
            "Epoch [314/1000], Loss: 133.0000\n",
            "Epoch [315/1000], Loss: 133.0000\n",
            "Epoch [316/1000], Loss: 133.0000\n",
            "Epoch [317/1000], Loss: 133.0000\n",
            "Epoch [318/1000], Loss: 133.0000\n",
            "Epoch [319/1000], Loss: 133.0000\n",
            "Epoch [320/1000], Loss: 133.0000\n",
            "Epoch [321/1000], Loss: 133.0000\n",
            "Epoch [322/1000], Loss: 133.0000\n",
            "Epoch [323/1000], Loss: 133.0000\n",
            "Epoch [324/1000], Loss: 133.0000\n",
            "Epoch [325/1000], Loss: 133.0000\n",
            "Epoch [326/1000], Loss: 133.0000\n",
            "Epoch [327/1000], Loss: 133.0000\n",
            "Epoch [328/1000], Loss: 133.0000\n",
            "Epoch [329/1000], Loss: 133.0000\n",
            "Epoch [330/1000], Loss: 133.0000\n",
            "Epoch [331/1000], Loss: 133.0000\n",
            "Epoch [332/1000], Loss: 133.0000\n",
            "Epoch [333/1000], Loss: 133.0000\n",
            "Epoch [334/1000], Loss: 133.0000\n",
            "Epoch [335/1000], Loss: 133.0000\n",
            "Epoch [336/1000], Loss: 133.0000\n",
            "Epoch [337/1000], Loss: 133.0000\n",
            "Epoch [338/1000], Loss: 133.0000\n",
            "Epoch [339/1000], Loss: 133.0000\n",
            "Epoch [340/1000], Loss: 133.0000\n",
            "Epoch [341/1000], Loss: 133.0000\n",
            "Epoch [342/1000], Loss: 133.0000\n",
            "Epoch [343/1000], Loss: 133.0000\n",
            "Epoch [344/1000], Loss: 133.0000\n",
            "Epoch [345/1000], Loss: 133.0000\n",
            "Epoch [346/1000], Loss: 133.0000\n",
            "Epoch [347/1000], Loss: 133.0000\n",
            "Epoch [348/1000], Loss: 133.0000\n",
            "Epoch [349/1000], Loss: 133.0000\n",
            "Epoch [350/1000], Loss: 133.0000\n",
            "Epoch [351/1000], Loss: 133.0000\n",
            "Epoch [352/1000], Loss: 133.0000\n",
            "Epoch [353/1000], Loss: 133.0000\n",
            "Epoch [354/1000], Loss: 133.0000\n",
            "Epoch [355/1000], Loss: 133.0000\n",
            "Epoch [356/1000], Loss: 133.0000\n",
            "Epoch [357/1000], Loss: 133.0000\n",
            "Epoch [358/1000], Loss: 133.0000\n",
            "Epoch [359/1000], Loss: 133.0000\n",
            "Epoch [360/1000], Loss: 133.0000\n",
            "Epoch [361/1000], Loss: 133.0000\n",
            "Epoch [362/1000], Loss: 133.0000\n",
            "Epoch [363/1000], Loss: 133.0000\n",
            "Epoch [364/1000], Loss: 133.0000\n",
            "Epoch [365/1000], Loss: 133.0000\n",
            "Epoch [366/1000], Loss: 133.0000\n",
            "Epoch [367/1000], Loss: 133.0000\n",
            "Epoch [368/1000], Loss: 133.0000\n",
            "Epoch [369/1000], Loss: 133.0000\n",
            "Epoch [370/1000], Loss: 133.0000\n",
            "Epoch [371/1000], Loss: 133.0000\n",
            "Epoch [372/1000], Loss: 133.0000\n",
            "Epoch [373/1000], Loss: 133.0000\n",
            "Epoch [374/1000], Loss: 133.0000\n",
            "Epoch [375/1000], Loss: 133.0000\n",
            "Epoch [376/1000], Loss: 133.0000\n",
            "Epoch [377/1000], Loss: 133.0000\n",
            "Epoch [378/1000], Loss: 133.0000\n",
            "Epoch [379/1000], Loss: 133.0000\n",
            "Epoch [380/1000], Loss: 133.0000\n",
            "Epoch [381/1000], Loss: 133.0000\n",
            "Epoch [382/1000], Loss: 133.0000\n",
            "Epoch [383/1000], Loss: 133.0000\n",
            "Epoch [384/1000], Loss: 133.0000\n",
            "Epoch [385/1000], Loss: 133.0000\n",
            "Epoch [386/1000], Loss: 133.0000\n",
            "Epoch [387/1000], Loss: 133.0000\n",
            "Epoch [388/1000], Loss: 133.0000\n",
            "Epoch [389/1000], Loss: 133.0000\n",
            "Epoch [390/1000], Loss: 133.0000\n",
            "Epoch [391/1000], Loss: 133.0000\n",
            "Epoch [392/1000], Loss: 133.0000\n",
            "Epoch [393/1000], Loss: 133.0000\n",
            "Epoch [394/1000], Loss: 133.0000\n",
            "Epoch [395/1000], Loss: 133.0000\n",
            "Epoch [396/1000], Loss: 133.0000\n",
            "Epoch [397/1000], Loss: 133.0000\n",
            "Epoch [398/1000], Loss: 133.0000\n",
            "Epoch [399/1000], Loss: 133.0000\n",
            "Epoch [400/1000], Loss: 133.0000\n",
            "Epoch [401/1000], Loss: 133.0000\n",
            "Epoch [402/1000], Loss: 133.0000\n",
            "Epoch [403/1000], Loss: 133.0000\n",
            "Epoch [404/1000], Loss: 133.0000\n",
            "Epoch [405/1000], Loss: 133.0000\n",
            "Epoch [406/1000], Loss: 133.0000\n",
            "Epoch [407/1000], Loss: 133.0000\n",
            "Epoch [408/1000], Loss: 133.0000\n",
            "Epoch [409/1000], Loss: 133.0000\n",
            "Epoch [410/1000], Loss: 133.0000\n",
            "Epoch [411/1000], Loss: 133.0000\n",
            "Epoch [412/1000], Loss: 133.0000\n",
            "Epoch [413/1000], Loss: 133.0000\n",
            "Epoch [414/1000], Loss: 133.0000\n",
            "Epoch [415/1000], Loss: 133.0000\n",
            "Epoch [416/1000], Loss: 133.0000\n",
            "Epoch [417/1000], Loss: 133.0000\n",
            "Epoch [418/1000], Loss: 133.0000\n",
            "Epoch [419/1000], Loss: 133.0000\n",
            "Epoch [420/1000], Loss: 133.0000\n",
            "Epoch [421/1000], Loss: 133.0000\n",
            "Epoch [422/1000], Loss: 133.0000\n",
            "Epoch [423/1000], Loss: 133.0000\n",
            "Epoch [424/1000], Loss: 133.0000\n",
            "Epoch [425/1000], Loss: 133.0000\n",
            "Epoch [426/1000], Loss: 133.0000\n",
            "Epoch [427/1000], Loss: 133.0000\n",
            "Epoch [428/1000], Loss: 133.0000\n",
            "Epoch [429/1000], Loss: 133.0000\n",
            "Epoch [430/1000], Loss: 133.0000\n",
            "Epoch [431/1000], Loss: 133.0000\n",
            "Epoch [432/1000], Loss: 133.0000\n",
            "Epoch [433/1000], Loss: 133.0000\n",
            "Epoch [434/1000], Loss: 133.0000\n",
            "Epoch [435/1000], Loss: 133.0000\n",
            "Epoch [436/1000], Loss: 133.0000\n",
            "Epoch [437/1000], Loss: 133.0000\n",
            "Epoch [438/1000], Loss: 133.0000\n",
            "Epoch [439/1000], Loss: 133.0000\n",
            "Epoch [440/1000], Loss: 133.0000\n",
            "Epoch [441/1000], Loss: 133.0000\n",
            "Epoch [442/1000], Loss: 133.0000\n",
            "Epoch [443/1000], Loss: 133.0000\n",
            "Epoch [444/1000], Loss: 133.0000\n",
            "Epoch [445/1000], Loss: 133.0000\n",
            "Epoch [446/1000], Loss: 133.0000\n",
            "Epoch [447/1000], Loss: 133.0000\n",
            "Epoch [448/1000], Loss: 133.0000\n",
            "Epoch [449/1000], Loss: 133.0000\n",
            "Epoch [450/1000], Loss: 133.0000\n",
            "Epoch [451/1000], Loss: 133.0000\n",
            "Epoch [452/1000], Loss: 133.0000\n",
            "Epoch [453/1000], Loss: 133.0000\n",
            "Epoch [454/1000], Loss: 133.0000\n",
            "Epoch [455/1000], Loss: 133.0000\n",
            "Epoch [456/1000], Loss: 133.0000\n",
            "Epoch [457/1000], Loss: 133.0000\n",
            "Epoch [458/1000], Loss: 133.0000\n",
            "Epoch [459/1000], Loss: 133.0000\n",
            "Epoch [460/1000], Loss: 133.0000\n",
            "Epoch [461/1000], Loss: 133.0000\n",
            "Epoch [462/1000], Loss: 133.0000\n",
            "Epoch [463/1000], Loss: 133.0000\n",
            "Epoch [464/1000], Loss: 133.0000\n",
            "Epoch [465/1000], Loss: 133.0000\n",
            "Epoch [466/1000], Loss: 133.0000\n",
            "Epoch [467/1000], Loss: 133.0000\n",
            "Epoch [468/1000], Loss: 133.0000\n",
            "Epoch [469/1000], Loss: 133.0000\n",
            "Epoch [470/1000], Loss: 133.0000\n",
            "Epoch [471/1000], Loss: 133.0000\n",
            "Epoch [472/1000], Loss: 133.0000\n",
            "Epoch [473/1000], Loss: 133.0000\n",
            "Epoch [474/1000], Loss: 133.0000\n",
            "Epoch [475/1000], Loss: 133.0000\n",
            "Epoch [476/1000], Loss: 133.0000\n",
            "Epoch [477/1000], Loss: 133.0000\n",
            "Epoch [478/1000], Loss: 133.0000\n",
            "Epoch [479/1000], Loss: 133.0000\n",
            "Epoch [480/1000], Loss: 133.0000\n",
            "Epoch [481/1000], Loss: 133.0000\n",
            "Epoch [482/1000], Loss: 133.0000\n",
            "Epoch [483/1000], Loss: 133.0000\n",
            "Epoch [484/1000], Loss: 133.0000\n",
            "Epoch [485/1000], Loss: 133.0000\n",
            "Epoch [486/1000], Loss: 133.0000\n",
            "Epoch [487/1000], Loss: 133.0000\n",
            "Epoch [488/1000], Loss: 133.0000\n",
            "Epoch [489/1000], Loss: 133.0000\n",
            "Epoch [490/1000], Loss: 133.0000\n",
            "Epoch [491/1000], Loss: 133.0000\n",
            "Epoch [492/1000], Loss: 133.0000\n",
            "Epoch [493/1000], Loss: 133.0000\n",
            "Epoch [494/1000], Loss: 133.0000\n",
            "Epoch [495/1000], Loss: 133.0000\n",
            "Epoch [496/1000], Loss: 133.0000\n",
            "Epoch [497/1000], Loss: 133.0000\n",
            "Epoch [498/1000], Loss: 133.0000\n",
            "Epoch [499/1000], Loss: 133.0000\n",
            "Epoch [500/1000], Loss: 133.0000\n",
            "Epoch [501/1000], Loss: 133.0000\n",
            "Epoch [502/1000], Loss: 133.0000\n",
            "Epoch [503/1000], Loss: 133.0000\n",
            "Epoch [504/1000], Loss: 133.0000\n",
            "Epoch [505/1000], Loss: 133.0000\n",
            "Epoch [506/1000], Loss: 133.0000\n",
            "Epoch [507/1000], Loss: 133.0000\n",
            "Epoch [508/1000], Loss: 133.0000\n",
            "Epoch [509/1000], Loss: 133.0000\n",
            "Epoch [510/1000], Loss: 133.0000\n",
            "Epoch [511/1000], Loss: 133.0000\n",
            "Epoch [512/1000], Loss: 133.0000\n",
            "Epoch [513/1000], Loss: 133.0000\n",
            "Epoch [514/1000], Loss: 133.0000\n",
            "Epoch [515/1000], Loss: 133.0000\n",
            "Epoch [516/1000], Loss: 133.0000\n",
            "Epoch [517/1000], Loss: 133.0000\n",
            "Epoch [518/1000], Loss: 133.0000\n",
            "Epoch [519/1000], Loss: 133.0000\n",
            "Epoch [520/1000], Loss: 133.0000\n",
            "Epoch [521/1000], Loss: 133.0000\n",
            "Epoch [522/1000], Loss: 133.0000\n",
            "Epoch [523/1000], Loss: 133.0000\n",
            "Epoch [524/1000], Loss: 133.0000\n",
            "Epoch [525/1000], Loss: 133.0000\n",
            "Epoch [526/1000], Loss: 133.0000\n",
            "Epoch [527/1000], Loss: 133.0000\n",
            "Epoch [528/1000], Loss: 133.0000\n",
            "Epoch [529/1000], Loss: 133.0000\n",
            "Epoch [530/1000], Loss: 133.0000\n",
            "Epoch [531/1000], Loss: 133.0000\n",
            "Epoch [532/1000], Loss: 133.0000\n",
            "Epoch [533/1000], Loss: 133.0000\n",
            "Epoch [534/1000], Loss: 133.0000\n",
            "Epoch [535/1000], Loss: 133.0000\n",
            "Epoch [536/1000], Loss: 133.0000\n",
            "Epoch [537/1000], Loss: 133.0000\n",
            "Epoch [538/1000], Loss: 133.0000\n",
            "Epoch [539/1000], Loss: 133.0000\n",
            "Epoch [540/1000], Loss: 133.0000\n",
            "Epoch [541/1000], Loss: 133.0000\n",
            "Epoch [542/1000], Loss: 133.0000\n",
            "Epoch [543/1000], Loss: 133.0000\n",
            "Epoch [544/1000], Loss: 133.0000\n",
            "Epoch [545/1000], Loss: 133.0000\n",
            "Epoch [546/1000], Loss: 133.0000\n",
            "Epoch [547/1000], Loss: 133.0000\n",
            "Epoch [548/1000], Loss: 133.0000\n",
            "Epoch [549/1000], Loss: 133.0000\n",
            "Epoch [550/1000], Loss: 133.0000\n",
            "Epoch [551/1000], Loss: 133.0000\n",
            "Epoch [552/1000], Loss: 133.0000\n",
            "Epoch [553/1000], Loss: 133.0000\n",
            "Epoch [554/1000], Loss: 133.0000\n",
            "Epoch [555/1000], Loss: 133.0000\n",
            "Epoch [556/1000], Loss: 133.0000\n",
            "Epoch [557/1000], Loss: 133.0000\n",
            "Epoch [558/1000], Loss: 133.0000\n",
            "Epoch [559/1000], Loss: 133.0000\n",
            "Epoch [560/1000], Loss: 133.0000\n",
            "Epoch [561/1000], Loss: 133.0000\n",
            "Epoch [562/1000], Loss: 133.0000\n",
            "Epoch [563/1000], Loss: 133.0000\n",
            "Epoch [564/1000], Loss: 133.0000\n",
            "Epoch [565/1000], Loss: 133.0000\n",
            "Epoch [566/1000], Loss: 133.0000\n",
            "Epoch [567/1000], Loss: 133.0000\n",
            "Epoch [568/1000], Loss: 133.0000\n",
            "Epoch [569/1000], Loss: 133.0000\n",
            "Epoch [570/1000], Loss: 133.0000\n",
            "Epoch [571/1000], Loss: 133.0000\n",
            "Epoch [572/1000], Loss: 133.0000\n",
            "Epoch [573/1000], Loss: 133.0000\n",
            "Epoch [574/1000], Loss: 133.0000\n",
            "Epoch [575/1000], Loss: 133.0000\n",
            "Epoch [576/1000], Loss: 133.0000\n",
            "Epoch [577/1000], Loss: 133.0000\n",
            "Epoch [578/1000], Loss: 133.0000\n",
            "Epoch [579/1000], Loss: 133.0000\n",
            "Epoch [580/1000], Loss: 133.0000\n",
            "Epoch [581/1000], Loss: 133.0000\n",
            "Epoch [582/1000], Loss: 133.0000\n",
            "Epoch [583/1000], Loss: 133.0000\n",
            "Epoch [584/1000], Loss: 133.0000\n",
            "Epoch [585/1000], Loss: 133.0000\n",
            "Epoch [586/1000], Loss: 133.0000\n",
            "Epoch [587/1000], Loss: 133.0000\n",
            "Epoch [588/1000], Loss: 133.0000\n",
            "Epoch [589/1000], Loss: 133.0000\n",
            "Epoch [590/1000], Loss: 133.0000\n",
            "Epoch [591/1000], Loss: 133.0000\n",
            "Epoch [592/1000], Loss: 133.0000\n",
            "Epoch [593/1000], Loss: 133.0000\n",
            "Epoch [594/1000], Loss: 133.0000\n",
            "Epoch [595/1000], Loss: 133.0000\n",
            "Epoch [596/1000], Loss: 133.0000\n",
            "Epoch [597/1000], Loss: 133.0000\n",
            "Epoch [598/1000], Loss: 133.0000\n",
            "Epoch [599/1000], Loss: 133.0000\n",
            "Epoch [600/1000], Loss: 133.0000\n",
            "Epoch [601/1000], Loss: 133.0000\n",
            "Epoch [602/1000], Loss: 133.0000\n",
            "Epoch [603/1000], Loss: 133.0000\n",
            "Epoch [604/1000], Loss: 133.0000\n",
            "Epoch [605/1000], Loss: 133.0000\n",
            "Epoch [606/1000], Loss: 133.0000\n",
            "Epoch [607/1000], Loss: 133.0000\n",
            "Epoch [608/1000], Loss: 133.0000\n",
            "Epoch [609/1000], Loss: 133.0000\n",
            "Epoch [610/1000], Loss: 133.0000\n",
            "Epoch [611/1000], Loss: 133.0000\n",
            "Epoch [612/1000], Loss: 133.0000\n",
            "Epoch [613/1000], Loss: 133.0000\n",
            "Epoch [614/1000], Loss: 133.0000\n",
            "Epoch [615/1000], Loss: 133.0000\n",
            "Epoch [616/1000], Loss: 133.0000\n",
            "Epoch [617/1000], Loss: 133.0000\n",
            "Epoch [618/1000], Loss: 133.0000\n",
            "Epoch [619/1000], Loss: 133.0000\n",
            "Epoch [620/1000], Loss: 133.0000\n",
            "Epoch [621/1000], Loss: 133.0000\n",
            "Epoch [622/1000], Loss: 133.0000\n",
            "Epoch [623/1000], Loss: 133.0000\n",
            "Epoch [624/1000], Loss: 133.0000\n",
            "Epoch [625/1000], Loss: 133.0000\n",
            "Epoch [626/1000], Loss: 133.0000\n",
            "Epoch [627/1000], Loss: 133.0000\n",
            "Epoch [628/1000], Loss: 133.0000\n",
            "Epoch [629/1000], Loss: 133.0000\n",
            "Epoch [630/1000], Loss: 133.0000\n",
            "Epoch [631/1000], Loss: 133.0000\n",
            "Epoch [632/1000], Loss: 133.0000\n",
            "Epoch [633/1000], Loss: 133.0000\n",
            "Epoch [634/1000], Loss: 133.0000\n",
            "Epoch [635/1000], Loss: 133.0000\n",
            "Epoch [636/1000], Loss: 133.0000\n",
            "Epoch [637/1000], Loss: 133.0000\n",
            "Epoch [638/1000], Loss: 133.0000\n",
            "Epoch [639/1000], Loss: 133.0000\n",
            "Epoch [640/1000], Loss: 133.0000\n",
            "Epoch [641/1000], Loss: 133.0000\n",
            "Epoch [642/1000], Loss: 133.0000\n",
            "Epoch [643/1000], Loss: 133.0000\n",
            "Epoch [644/1000], Loss: 133.0000\n",
            "Epoch [645/1000], Loss: 133.0000\n",
            "Epoch [646/1000], Loss: 133.0000\n",
            "Epoch [647/1000], Loss: 133.0000\n",
            "Epoch [648/1000], Loss: 133.0000\n",
            "Epoch [649/1000], Loss: 133.0000\n",
            "Epoch [650/1000], Loss: 133.0000\n",
            "Epoch [651/1000], Loss: 133.0000\n",
            "Epoch [652/1000], Loss: 133.0000\n",
            "Epoch [653/1000], Loss: 133.0000\n",
            "Epoch [654/1000], Loss: 133.0000\n",
            "Epoch [655/1000], Loss: 133.0000\n",
            "Epoch [656/1000], Loss: 133.0000\n",
            "Epoch [657/1000], Loss: 133.0000\n",
            "Epoch [658/1000], Loss: 133.0000\n",
            "Epoch [659/1000], Loss: 133.0000\n",
            "Epoch [660/1000], Loss: 133.0000\n",
            "Epoch [661/1000], Loss: 133.0000\n",
            "Epoch [662/1000], Loss: 133.0000\n",
            "Epoch [663/1000], Loss: 133.0000\n",
            "Epoch [664/1000], Loss: 133.0000\n",
            "Epoch [665/1000], Loss: 133.0000\n",
            "Epoch [666/1000], Loss: 133.0000\n",
            "Epoch [667/1000], Loss: 133.0000\n",
            "Epoch [668/1000], Loss: 133.0000\n",
            "Epoch [669/1000], Loss: 133.0000\n",
            "Epoch [670/1000], Loss: 133.0000\n",
            "Epoch [671/1000], Loss: 133.0000\n",
            "Epoch [672/1000], Loss: 133.0000\n",
            "Epoch [673/1000], Loss: 133.0000\n",
            "Epoch [674/1000], Loss: 133.0000\n",
            "Epoch [675/1000], Loss: 133.0000\n",
            "Epoch [676/1000], Loss: 133.0000\n",
            "Epoch [677/1000], Loss: 133.0000\n",
            "Epoch [678/1000], Loss: 133.0000\n",
            "Epoch [679/1000], Loss: 133.0000\n",
            "Epoch [680/1000], Loss: 133.0000\n",
            "Epoch [681/1000], Loss: 133.0000\n",
            "Epoch [682/1000], Loss: 133.0000\n",
            "Epoch [683/1000], Loss: 133.0000\n",
            "Epoch [684/1000], Loss: 133.0000\n",
            "Epoch [685/1000], Loss: 133.0000\n",
            "Epoch [686/1000], Loss: 133.0000\n",
            "Epoch [687/1000], Loss: 133.0000\n",
            "Epoch [688/1000], Loss: 133.0000\n",
            "Epoch [689/1000], Loss: 133.0000\n",
            "Epoch [690/1000], Loss: 133.0000\n",
            "Epoch [691/1000], Loss: 133.0000\n",
            "Epoch [692/1000], Loss: 133.0000\n",
            "Epoch [693/1000], Loss: 133.0000\n",
            "Epoch [694/1000], Loss: 133.0000\n",
            "Epoch [695/1000], Loss: 133.0000\n",
            "Epoch [696/1000], Loss: 133.0000\n",
            "Epoch [697/1000], Loss: 133.0000\n",
            "Epoch [698/1000], Loss: 133.0000\n",
            "Epoch [699/1000], Loss: 133.0000\n",
            "Epoch [700/1000], Loss: 133.0000\n",
            "Epoch [701/1000], Loss: 133.0000\n",
            "Epoch [702/1000], Loss: 133.0000\n",
            "Epoch [703/1000], Loss: 133.0000\n",
            "Epoch [704/1000], Loss: 133.0000\n",
            "Epoch [705/1000], Loss: 133.0000\n",
            "Epoch [706/1000], Loss: 133.0000\n",
            "Epoch [707/1000], Loss: 133.0000\n",
            "Epoch [708/1000], Loss: 133.0000\n",
            "Epoch [709/1000], Loss: 133.0000\n",
            "Epoch [710/1000], Loss: 133.0000\n",
            "Epoch [711/1000], Loss: 133.0000\n",
            "Epoch [712/1000], Loss: 133.0000\n",
            "Epoch [713/1000], Loss: 133.0000\n",
            "Epoch [714/1000], Loss: 133.0000\n",
            "Epoch [715/1000], Loss: 133.0000\n",
            "Epoch [716/1000], Loss: 133.0000\n",
            "Epoch [717/1000], Loss: 133.0000\n",
            "Epoch [718/1000], Loss: 133.0000\n",
            "Epoch [719/1000], Loss: 133.0000\n",
            "Epoch [720/1000], Loss: 133.0000\n",
            "Epoch [721/1000], Loss: 133.0000\n",
            "Epoch [722/1000], Loss: 133.0000\n",
            "Epoch [723/1000], Loss: 133.0000\n",
            "Epoch [724/1000], Loss: 133.0000\n",
            "Epoch [725/1000], Loss: 133.0000\n",
            "Epoch [726/1000], Loss: 133.0000\n",
            "Epoch [727/1000], Loss: 133.0000\n",
            "Epoch [728/1000], Loss: 133.0000\n",
            "Epoch [729/1000], Loss: 133.0000\n",
            "Epoch [730/1000], Loss: 133.0000\n",
            "Epoch [731/1000], Loss: 133.0000\n",
            "Epoch [732/1000], Loss: 133.0000\n",
            "Epoch [733/1000], Loss: 133.0000\n",
            "Epoch [734/1000], Loss: 133.0000\n",
            "Epoch [735/1000], Loss: 133.0000\n",
            "Epoch [736/1000], Loss: 133.0000\n",
            "Epoch [737/1000], Loss: 133.0000\n",
            "Epoch [738/1000], Loss: 133.0000\n",
            "Epoch [739/1000], Loss: 133.0000\n",
            "Epoch [740/1000], Loss: 133.0000\n",
            "Epoch [741/1000], Loss: 133.0000\n",
            "Epoch [742/1000], Loss: 133.0000\n",
            "Epoch [743/1000], Loss: 133.0000\n",
            "Epoch [744/1000], Loss: 133.0000\n",
            "Epoch [745/1000], Loss: 133.0000\n",
            "Epoch [746/1000], Loss: 133.0000\n",
            "Epoch [747/1000], Loss: 133.0000\n",
            "Epoch [748/1000], Loss: 133.0000\n",
            "Epoch [749/1000], Loss: 133.0000\n",
            "Epoch [750/1000], Loss: 133.0000\n",
            "Epoch [751/1000], Loss: 133.0000\n",
            "Epoch [752/1000], Loss: 133.0000\n",
            "Epoch [753/1000], Loss: 133.0000\n",
            "Epoch [754/1000], Loss: 133.0000\n",
            "Epoch [755/1000], Loss: 133.0000\n",
            "Epoch [756/1000], Loss: 133.0000\n",
            "Epoch [757/1000], Loss: 133.0000\n",
            "Epoch [758/1000], Loss: 133.0000\n",
            "Epoch [759/1000], Loss: 133.0000\n",
            "Epoch [760/1000], Loss: 133.0000\n",
            "Epoch [761/1000], Loss: 133.0000\n",
            "Epoch [762/1000], Loss: 133.0000\n",
            "Epoch [763/1000], Loss: 133.0000\n",
            "Epoch [764/1000], Loss: 133.0000\n",
            "Epoch [765/1000], Loss: 133.0000\n",
            "Epoch [766/1000], Loss: 133.0000\n",
            "Epoch [767/1000], Loss: 133.0000\n",
            "Epoch [768/1000], Loss: 133.0000\n",
            "Epoch [769/1000], Loss: 133.0000\n",
            "Epoch [770/1000], Loss: 133.0000\n",
            "Epoch [771/1000], Loss: 133.0000\n",
            "Epoch [772/1000], Loss: 133.0000\n",
            "Epoch [773/1000], Loss: 133.0000\n",
            "Epoch [774/1000], Loss: 133.0000\n",
            "Epoch [775/1000], Loss: 133.0000\n",
            "Epoch [776/1000], Loss: 133.0000\n",
            "Epoch [777/1000], Loss: 133.0000\n",
            "Epoch [778/1000], Loss: 133.0000\n",
            "Epoch [779/1000], Loss: 133.0000\n",
            "Epoch [780/1000], Loss: 133.0000\n",
            "Epoch [781/1000], Loss: 133.0000\n",
            "Epoch [782/1000], Loss: 133.0000\n",
            "Epoch [783/1000], Loss: 133.0000\n",
            "Epoch [784/1000], Loss: 133.0000\n",
            "Epoch [785/1000], Loss: 133.0000\n",
            "Epoch [786/1000], Loss: 133.0000\n",
            "Epoch [787/1000], Loss: 133.0000\n",
            "Epoch [788/1000], Loss: 133.0000\n",
            "Epoch [789/1000], Loss: 133.0000\n",
            "Epoch [790/1000], Loss: 133.0000\n",
            "Epoch [791/1000], Loss: 133.0000\n",
            "Epoch [792/1000], Loss: 133.0000\n",
            "Epoch [793/1000], Loss: 133.0000\n",
            "Epoch [794/1000], Loss: 133.0000\n",
            "Epoch [795/1000], Loss: 133.0000\n",
            "Epoch [796/1000], Loss: 133.0000\n",
            "Epoch [797/1000], Loss: 133.0000\n",
            "Epoch [798/1000], Loss: 133.0000\n",
            "Epoch [799/1000], Loss: 133.0000\n",
            "Epoch [800/1000], Loss: 133.0000\n",
            "Epoch [801/1000], Loss: 133.0000\n",
            "Epoch [802/1000], Loss: 133.0000\n",
            "Epoch [803/1000], Loss: 133.0000\n",
            "Epoch [804/1000], Loss: 133.0000\n",
            "Epoch [805/1000], Loss: 133.0000\n",
            "Epoch [806/1000], Loss: 133.0000\n",
            "Epoch [807/1000], Loss: 133.0000\n",
            "Epoch [808/1000], Loss: 133.0000\n",
            "Epoch [809/1000], Loss: 133.0000\n",
            "Epoch [810/1000], Loss: 133.0000\n",
            "Epoch [811/1000], Loss: 133.0000\n",
            "Epoch [812/1000], Loss: 133.0000\n",
            "Epoch [813/1000], Loss: 133.0000\n",
            "Epoch [814/1000], Loss: 133.0000\n",
            "Epoch [815/1000], Loss: 133.0000\n",
            "Epoch [816/1000], Loss: 133.0000\n",
            "Epoch [817/1000], Loss: 133.0000\n",
            "Epoch [818/1000], Loss: 133.0000\n",
            "Epoch [819/1000], Loss: 133.0000\n",
            "Epoch [820/1000], Loss: 133.0000\n",
            "Epoch [821/1000], Loss: 133.0000\n",
            "Epoch [822/1000], Loss: 133.0000\n",
            "Epoch [823/1000], Loss: 133.0000\n",
            "Epoch [824/1000], Loss: 133.0000\n",
            "Epoch [825/1000], Loss: 133.0000\n",
            "Epoch [826/1000], Loss: 133.0000\n",
            "Epoch [827/1000], Loss: 133.0000\n",
            "Epoch [828/1000], Loss: 133.0000\n",
            "Epoch [829/1000], Loss: 133.0000\n",
            "Epoch [830/1000], Loss: 133.0000\n",
            "Epoch [831/1000], Loss: 133.0000\n",
            "Epoch [832/1000], Loss: 133.0000\n",
            "Epoch [833/1000], Loss: 133.0000\n",
            "Epoch [834/1000], Loss: 133.0000\n",
            "Epoch [835/1000], Loss: 133.0000\n",
            "Epoch [836/1000], Loss: 133.0000\n",
            "Epoch [837/1000], Loss: 133.0000\n",
            "Epoch [838/1000], Loss: 133.0000\n",
            "Epoch [839/1000], Loss: 133.0000\n",
            "Epoch [840/1000], Loss: 133.0000\n",
            "Epoch [841/1000], Loss: 133.0000\n",
            "Epoch [842/1000], Loss: 133.0000\n",
            "Epoch [843/1000], Loss: 133.0000\n",
            "Epoch [844/1000], Loss: 133.0000\n",
            "Epoch [845/1000], Loss: 133.0000\n",
            "Epoch [846/1000], Loss: 133.0000\n",
            "Epoch [847/1000], Loss: 133.0000\n",
            "Epoch [848/1000], Loss: 133.0000\n",
            "Epoch [849/1000], Loss: 133.0000\n",
            "Epoch [850/1000], Loss: 133.0000\n",
            "Epoch [851/1000], Loss: 133.0000\n",
            "Epoch [852/1000], Loss: 133.0000\n",
            "Epoch [853/1000], Loss: 133.0000\n",
            "Epoch [854/1000], Loss: 133.0000\n",
            "Epoch [855/1000], Loss: 133.0000\n",
            "Epoch [856/1000], Loss: 133.0000\n",
            "Epoch [857/1000], Loss: 133.0000\n",
            "Epoch [858/1000], Loss: 133.0000\n",
            "Epoch [859/1000], Loss: 133.0000\n",
            "Epoch [860/1000], Loss: 133.0000\n",
            "Epoch [861/1000], Loss: 133.0000\n",
            "Epoch [862/1000], Loss: 133.0000\n",
            "Epoch [863/1000], Loss: 133.0000\n",
            "Epoch [864/1000], Loss: 133.0000\n",
            "Epoch [865/1000], Loss: 133.0000\n",
            "Epoch [866/1000], Loss: 133.0000\n",
            "Epoch [867/1000], Loss: 133.0000\n",
            "Epoch [868/1000], Loss: 133.0000\n",
            "Epoch [869/1000], Loss: 133.0000\n",
            "Epoch [870/1000], Loss: 133.0000\n",
            "Epoch [871/1000], Loss: 133.0000\n",
            "Epoch [872/1000], Loss: 133.0000\n",
            "Epoch [873/1000], Loss: 133.0000\n",
            "Epoch [874/1000], Loss: 133.0000\n",
            "Epoch [875/1000], Loss: 133.0000\n",
            "Epoch [876/1000], Loss: 133.0000\n",
            "Epoch [877/1000], Loss: 133.0000\n",
            "Epoch [878/1000], Loss: 133.0000\n",
            "Epoch [879/1000], Loss: 133.0000\n",
            "Epoch [880/1000], Loss: 133.0000\n",
            "Epoch [881/1000], Loss: 133.0000\n",
            "Epoch [882/1000], Loss: 133.0000\n",
            "Epoch [883/1000], Loss: 133.0000\n",
            "Epoch [884/1000], Loss: 133.0000\n",
            "Epoch [885/1000], Loss: 133.0000\n",
            "Epoch [886/1000], Loss: 133.0000\n",
            "Epoch [887/1000], Loss: 133.0000\n",
            "Epoch [888/1000], Loss: 133.0000\n",
            "Epoch [889/1000], Loss: 133.0000\n",
            "Epoch [890/1000], Loss: 133.0000\n",
            "Epoch [891/1000], Loss: 133.0000\n",
            "Epoch [892/1000], Loss: 133.0000\n",
            "Epoch [893/1000], Loss: 133.0000\n",
            "Epoch [894/1000], Loss: 133.0000\n",
            "Epoch [895/1000], Loss: 133.0000\n",
            "Epoch [896/1000], Loss: 133.0000\n",
            "Epoch [897/1000], Loss: 133.0000\n",
            "Epoch [898/1000], Loss: 133.0000\n",
            "Epoch [899/1000], Loss: 133.0000\n",
            "Epoch [900/1000], Loss: 133.0000\n",
            "Epoch [901/1000], Loss: 133.0000\n",
            "Epoch [902/1000], Loss: 133.0000\n",
            "Epoch [903/1000], Loss: 133.0000\n",
            "Epoch [904/1000], Loss: 133.0000\n",
            "Epoch [905/1000], Loss: 133.0000\n",
            "Epoch [906/1000], Loss: 133.0000\n",
            "Epoch [907/1000], Loss: 133.0000\n",
            "Epoch [908/1000], Loss: 133.0000\n",
            "Epoch [909/1000], Loss: 133.0000\n",
            "Epoch [910/1000], Loss: 133.0000\n",
            "Epoch [911/1000], Loss: 133.0000\n",
            "Epoch [912/1000], Loss: 133.0000\n",
            "Epoch [913/1000], Loss: 133.0000\n",
            "Epoch [914/1000], Loss: 133.0000\n",
            "Epoch [915/1000], Loss: 133.0000\n",
            "Epoch [916/1000], Loss: 133.0000\n",
            "Epoch [917/1000], Loss: 133.0000\n",
            "Epoch [918/1000], Loss: 133.0000\n",
            "Epoch [919/1000], Loss: 133.0000\n",
            "Epoch [920/1000], Loss: 133.0000\n",
            "Epoch [921/1000], Loss: 133.0000\n",
            "Epoch [922/1000], Loss: 133.0000\n",
            "Epoch [923/1000], Loss: 133.0000\n",
            "Epoch [924/1000], Loss: 133.0000\n",
            "Epoch [925/1000], Loss: 133.0000\n",
            "Epoch [926/1000], Loss: 133.0000\n",
            "Epoch [927/1000], Loss: 133.0000\n",
            "Epoch [928/1000], Loss: 133.0000\n",
            "Epoch [929/1000], Loss: 133.0000\n",
            "Epoch [930/1000], Loss: 133.0000\n",
            "Epoch [931/1000], Loss: 133.0000\n",
            "Epoch [932/1000], Loss: 133.0000\n",
            "Epoch [933/1000], Loss: 133.0000\n",
            "Epoch [934/1000], Loss: 133.0000\n",
            "Epoch [935/1000], Loss: 133.0000\n",
            "Epoch [936/1000], Loss: 133.0000\n",
            "Epoch [937/1000], Loss: 133.0000\n",
            "Epoch [938/1000], Loss: 133.0000\n",
            "Epoch [939/1000], Loss: 133.0000\n",
            "Epoch [940/1000], Loss: 133.0000\n",
            "Epoch [941/1000], Loss: 133.0000\n",
            "Epoch [942/1000], Loss: 133.0000\n",
            "Epoch [943/1000], Loss: 133.0000\n",
            "Epoch [944/1000], Loss: 133.0000\n",
            "Epoch [945/1000], Loss: 133.0000\n",
            "Epoch [946/1000], Loss: 133.0000\n",
            "Epoch [947/1000], Loss: 133.0000\n",
            "Epoch [948/1000], Loss: 133.0000\n",
            "Epoch [949/1000], Loss: 133.0000\n",
            "Epoch [950/1000], Loss: 133.0000\n",
            "Epoch [951/1000], Loss: 133.0000\n",
            "Epoch [952/1000], Loss: 133.0000\n",
            "Epoch [953/1000], Loss: 133.0000\n",
            "Epoch [954/1000], Loss: 133.0000\n",
            "Epoch [955/1000], Loss: 133.0000\n",
            "Epoch [956/1000], Loss: 133.0000\n",
            "Epoch [957/1000], Loss: 133.0000\n",
            "Epoch [958/1000], Loss: 133.0000\n",
            "Epoch [959/1000], Loss: 133.0000\n",
            "Epoch [960/1000], Loss: 133.0000\n",
            "Epoch [961/1000], Loss: 133.0000\n",
            "Epoch [962/1000], Loss: 133.0000\n",
            "Epoch [963/1000], Loss: 133.0000\n",
            "Epoch [964/1000], Loss: 133.0000\n",
            "Epoch [965/1000], Loss: 133.0000\n",
            "Epoch [966/1000], Loss: 133.0000\n",
            "Epoch [967/1000], Loss: 133.0000\n",
            "Epoch [968/1000], Loss: 133.0000\n",
            "Epoch [969/1000], Loss: 133.0000\n",
            "Epoch [970/1000], Loss: 133.0000\n",
            "Epoch [971/1000], Loss: 133.0000\n",
            "Epoch [972/1000], Loss: 133.0000\n",
            "Epoch [973/1000], Loss: 133.0000\n",
            "Epoch [974/1000], Loss: 133.0000\n",
            "Epoch [975/1000], Loss: 133.0000\n",
            "Epoch [976/1000], Loss: 133.0000\n",
            "Epoch [977/1000], Loss: 133.0000\n",
            "Epoch [978/1000], Loss: 133.0000\n",
            "Epoch [979/1000], Loss: 133.0000\n",
            "Epoch [980/1000], Loss: 133.0000\n",
            "Epoch [981/1000], Loss: 133.0000\n",
            "Epoch [982/1000], Loss: 133.0000\n",
            "Epoch [983/1000], Loss: 133.0000\n",
            "Epoch [984/1000], Loss: 133.0000\n",
            "Epoch [985/1000], Loss: 133.0000\n",
            "Epoch [986/1000], Loss: 133.0000\n",
            "Epoch [987/1000], Loss: 133.0000\n",
            "Epoch [988/1000], Loss: 133.0000\n",
            "Epoch [989/1000], Loss: 133.0000\n",
            "Epoch [990/1000], Loss: 133.0000\n",
            "Epoch [991/1000], Loss: 133.0000\n",
            "Epoch [992/1000], Loss: 133.0000\n",
            "Epoch [993/1000], Loss: 133.0000\n",
            "Epoch [994/1000], Loss: 133.0000\n",
            "Epoch [995/1000], Loss: 133.0000\n",
            "Epoch [996/1000], Loss: 133.0000\n",
            "Epoch [997/1000], Loss: 133.0000\n",
            "Epoch [998/1000], Loss: 133.0000\n",
            "Epoch [999/1000], Loss: 133.0000\n",
            "Epoch [1000/1000], Loss: 133.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ewmV0dRTgm"
      },
      "source": [
        "## CIFAR-10データローダーの作成\n",
        "\n",
        "### CIFAR-10とは\n",
        "\n",
        "- 合計で6万枚のRGB画像(32x32)を含むデータセット\n",
        "- クラス数は10クラス(各クラス学習用5000枚、評価用1000枚)\n",
        "\n",
        "### データセットの作成\n",
        "\n",
        "pytorchでは、torchvisionのライブラリを使用することで`torchvision.datasets.CIFAR10`のように事前に決められた[データセット](https://pytorch.org/vision/0.8/datasets.html)を使用することができます。\n",
        "\n",
        "`root`はデータセットを保存するパスを指定します。\n",
        "今回は`path`という変数を使用しているため、`path`を書き換えることでデータの保存先を各自変更してください。\n",
        "例えば、Google Driveの配下の`b3_proj_2023/data`に学習用データを保存する場合は`path`を\n",
        "```python\n",
        "path = \"./drive/My Drive/Colab Notebooks/b3_proj_2023/data\"\n",
        "```\n",
        "のように変更します。\n",
        "(このように自身のGoogle Drive上に保存することで次回以降にデータを再度ダウンロードする必要がなくなるので便利です。)\n",
        "\n",
        "`train=True|False`で学習データか評価データかを指定することができます。 \\\n",
        "また、`transform`でデータ変換やデータオーグメンテーションを指定することができます。\n",
        "データ変換は複数同時に適用することがほとんどで、そのような場合は`transforms.Compose`を使用して複数のデータ変換を一連の処理として指定します。\n",
        "```python\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    ...\n",
        "     ])\n",
        "```\n",
        "のようにpythonのリストで指定し、データを読み込む際に上から順番に実行されます。\n",
        "適用可能な関数の一覧は[こちら](https://pytorch.org/vision/main/transforms.html)を参照してください。\n",
        "\n",
        "## データローダーの作成\n",
        "\n",
        "データセットに含まれるデータ数は非常に大きく、一度にメモリに読み込むことは難しい場合が多いです。\\\n",
        "そのため、そこで一度に読み出すデータの数を減らすことで計算負荷を減らしています。\\\n",
        "これをミニバッチと呼び、ロードするデータ数をバッチサイズと呼びます。\n",
        "\n",
        "pytorchでは、こうしたバッチ処理を`DataLoader`というクラスを用いて行います。\\\n",
        "`DataLoader`は`batch_size`という引数で一度に読み出すデータの数を指定することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "pf7Pb9lsRTgm",
        "outputId": "edff79f1-50eb-4346-f8b9-68e1121caec1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'transforms' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-87f1df4a808e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## Define Augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m transform = transforms.Compose(\n\u001b[0m\u001b[1;32m      7\u001b[0m     [transforms.ToTensor(),\n\u001b[1;32m      8\u001b[0m      ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ],
      "source": [
        "## Please update the path to your own directory.\n",
        "# path=/path/to/your_own  # Uncomment this line\n",
        "path = '../../work/data/cifar10'\n",
        "\n",
        "## Define Augmentation\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     ])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root=path, train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root=path, train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7VXk9E8RTgm"
      },
      "source": [
        "### データセット・データローダーの確認\n",
        "\n",
        "先ほどのデータセットにどのようなデータが保存されているのか確認します。\n",
        "\n",
        "`images`は$[B\\times 3\\times 32\\times 32]$のテンソルでモデルの入力に使用します。\\\n",
        "また、`labels`は$[B]$のテンソルで学習のラベルとして使われます。ここで、$B$はバッチサイズを表します。\n",
        "\n",
        "下のセルを実行すると変数`batch_size`で指定した数と同じ枚数の画像が出力されるのがわかるかと思います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTFORUMQRTgn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "torch.manual_seed(15)\n",
        "\n",
        "def imshow(imgs, labels):\n",
        "    imgs = imgs / 2 + 0.5     # unnormalize\n",
        "    npimg = imgs.numpy()\n",
        "    for i, (img, label) in enumerate(zip(npimg, labels)):\n",
        "        plt.subplot(1, batch_size, i+1)\n",
        "        plt.grid(False)\n",
        "        plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "        label = label.item()\n",
        "        plt.title(f\"{classes[label]}({label})\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(images, labels)\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMZBU4k5RTgn"
      },
      "source": [
        "### 演習\n",
        "\n",
        "- データ変換に正規化を追加してください。\n",
        "- バッチサイズを4に変更した場合に出力がどう変化するか確認してください。\n",
        "- ランダムに上下を反転させるデータ変換を追加してください。(任意)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRebxlu9RTgo"
      },
      "source": [
        "## モデルの作成\n",
        "ここでは、推論に使用するモデルの作成を行います。\n",
        "\n",
        "モデルを作成する前にニューラルネットワークで広く用いられるモデルの構成要素について簡単に説明します。\n",
        "\n",
        "### 全結合層\n",
        "<img src=\"https://github.com/ArtIC-TITECH/b3-proj-2023/blob/main/resources/class_01/fully_connected.png?raw=1\" width=\"10%\">\n",
        "\n",
        "全結合層とは、入力と出力が全て重みによって結合されている層を指します。\\\n",
        "図では、一次元で表していますが、`pytorch`ではバッチ処理が行われるため実際は2次元の入力に対して処理が行われます。\\\n",
        "そのため、入力$\\mathbf{x} \\in \\mathbb{R}^{B\\times C_{\\rm in}}$に対して、重み$W\\in\\mathbb{R}^{C_{\\rm in}\\times C_{\\rm out}}$とバイアス$\\mathbf{B} \\in \\mathbb{R}^{B\\times C_{\\rm out}}$を用いて出力$y\\in\\mathbb{R}^{B\\times C_{\\rm out}}$は次のように計算されます。\n",
        "$$\\mathbf{y} = \\mathbf{x}\\mathbf{W} + \\mathbf{B}$$\n",
        "ここで、$B$はバッチサイズ、$C_{\\rm in}$、$C_{\\rm out}$は入力チャネル、出力チャネル数を表しています。\n",
        "\n",
        "`pytorch`では`nn.Linear`というクラスで実装されています。\\\n",
        "`in_features`で入力のチャネル数、`out_features`で出力のチャネル数を指定します。\n",
        "\n",
        "例えば、入力チャネルが$4$、出力チャネルが$10$の全結合層は次のように実装します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE857_JvRTgp"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "input = torch.rand((4, 3))\n",
        "print(f'Input Shape is {input.shape}')\n",
        "linear = nn.Linear(in_features=3, out_features=10)\n",
        "output = linear(input)\n",
        "print(f'Output Shape is {output.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11NDBGQRTgp"
      },
      "source": [
        "演習\n",
        "上のセルを修正して入力チャネルが5、出力チャネルが15の線形層を`nn.Linear`を用いて実装してください。\\\n",
        "このとき、入力のバッチサイズを4とし、線形層の入力チャネルと入力データのチャネル数を揃える必要があることに注意してください。\n",
        "\n",
        "\n",
        "### 畳み込み層\n",
        "<img src=\"https://github.com/ArtIC-TITECH/b3-proj-2023/blob/main/resources/class_01/conv1d.png?raw=1\" width=\"30%\">\n",
        "\n",
        "全結合層では全ての入力チャネルと出力チャネルが重みで結合されていたのに対し、畳み込み層では入力に対してカーネルをストライドさせることで出力を計算します。\\\n",
        "そのため、出力のサイズは、カーネルサイズ、ストライド、パディングによって決定されます。\\\n",
        "画像では1次元のデータに対する1次元の畳み込みを例にしていますが、画像のデータセットでは4次元の入力に対し2次元の畳み込みを行うことが一般的です。\\\n",
        "この場合カーネルは$[C_{\\rm out}, C_{\\rm in}, k_{\\rm h}, k_{\\rm w}]$の4次元のテンソルで定義され、このカーネルを画像の縦横方向にストライドさせることで計算を行います。\n",
        "\n",
        "例えば、入力サイズが$[4, 3, 10, 10]$のデータに対して、出力チャネル6、カーネルサイズ3、パディングおよびストライドが1の畳み込みは次のように実装されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_0CJFABRTgq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "input = torch.rand((4, 3, 10, 10))\n",
        "print(f'Input Shape is {input.shape}')\n",
        "conv2d = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, padding=1, stride=1)\n",
        "output = conv2d(input)\n",
        "print(f'Output Shape is {output.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwvG9d_1RTgr"
      },
      "source": [
        "演習\n",
        "上のセルを修正して入力チャネルが5、出力チャネルが10のカーネルサイズが5、パディングが0、ストライドが1のConvを実装してください。\\\n",
        "このとき、入力のバッチサイズを4とし、入力チャネルと入力データのチャネル数を揃える必要があることに注意してください。\\\n",
        "出力のサイズが変化していることを確認してください。\n",
        "\n",
        "### プーリング層\n",
        "\n",
        "プーリング層は畳み込み層と同様にカーネルを画像の$H$、$W$方向に動かしながら処理を行いますが、次の2点で畳み込み層と異なります。\n",
        "- チャネル間の情報を集約しない\n",
        "- カーネルは重みではなく決まった処理が行われる\n",
        "    - MaxPool2dでは、カーネル内の最大値が出力される\n",
        "    - AvgPool2dでは、カーネル内の平均値が出力される\n",
        "\n",
        "例えば、カーネルサイズ$2$、ストライド$2$のMaxPool2dは次のように使用します。\\\n",
        "入出力を比較すると$2\\times2$のカーネルの中から最大値が選ばれていることがわかります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i812vxPkRTgr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "input = torch.rand((4, 3, 8, 8))\n",
        "print(input[0][0])\n",
        "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "output = pool(input)\n",
        "\n",
        "print(output[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJIPPJZRTgs"
      },
      "source": [
        "### 活性化関数\n",
        "活性化関数とは、入力に対して非線形処理を適用しニューラルネットワークの表現力を向上させるために使用する関数です。\n",
        "\n",
        "活性化関数には様々な種類がありますが、今回の演習では`ReLU`関数を非線形関数として用います。\\\n",
        "次のコードは入力に対して`ReLU`を適用したもので、$0$以下の入力が全て$0$となることがわかります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICTQH-DNRTgs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "\n",
        "input = torch.randn(100,)\n",
        "act = nn.ReLU()\n",
        "output = act(input)\n",
        "plt.plot(input.sort().values, output.sort().values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNskxBt4RTgs"
      },
      "source": [
        "ここまでで説明した4つの構成要素を用いてニューラルネットワークは次のように実装されます。\\\n",
        "ここでは、畳み込み1層、プーリング層1層、全結合層1層のニューラルネットワークを実装しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0ljGwsRRTgt"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.act = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(in_features=6*14*14, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.act(self.conv1(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHlfGPcRTgt"
      },
      "source": [
        "### 実際にモデルを動かしてみよう\n",
        "\n",
        "ここでは乱数を入力としてモデルを動かしてみます。\n",
        "ニューラルネットワークは、入力サイズ$(B, C, H, W)$のテンソルを入力として、$(B, \\rm{Cls})$を出力とします。\\\n",
        "ここで$B$はバッチサイズ、$C$は入力チャネル数(RGB画像の場合は3チャネル)、$H$, $W$は画像の幅と高さになります。CIFAR-10データセットの場合は、$H=W=32$です。\\\n",
        "$\\rm{Cls}$は出力のクラス数を表し、CIFAR-10は10クラスを出力します。\\\n",
        "ここでは、$(B, C, H, W) = (1, 3, 32, 32)$の乱数を入力としてモデルの推論を行います。\\\n",
        "実際にコードを実行すると(1, 10)の出力が得られると思います。出力の各要素がそれぞれのクラスの予測値となります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmISu7WkRTgu"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(100)\n",
        "\n",
        "input = torch.rand((1, 3, 32, 32))\n",
        "net = Net()\n",
        "output = net(input)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dldlGdB0RTgu"
      },
      "source": [
        "演習\\\n",
        "`Net`クラスを修正して以下で定義されるモデルを実装してください。\\\n",
        "Conv2dは全て`padding=0`、`stride=1`で、MaxPool2dは`stride=2`で実装してください。\\\n",
        "それぞれの層のカーネルサイズは、テーブルの`Kernel Shape`を元に決定してください。\\\n",
        "それぞれの層のチャネル数は、入力チャネル、出力チャネルを参考に実装してください。\\\n",
        "最終層以外のConv, Linear層の後には\n",
        "\n",
        "```\n",
        "===================================================================================================================\n",
        "Layer (type:depth-idx)                   Input Shape               Output Shape              Kernel Shape\n",
        "===================================================================================================================\n",
        "Net                                      [4, 3, 32, 32]            [4, 10]                   --\n",
        "├─Conv2d: 1-1                            [4, 3, 32, 32]            [4, 6, 28, 28]            [5, 5]\n",
        "│    └─weight                                                                                [3, 6, 5, 5]\n",
        "│    └─bias                                                                                  [6]\n",
        "|-ReLU                                   [4, 6, 28, 28]            [4, 6, 28, 28]\n",
        "├─MaxPool2d: 1-2                         [4, 6, 28, 28]            [4, 6, 14, 14]            2\n",
        "├─Conv2d: 1-3                            [4, 6, 14, 14]            [4, 16, 10, 10]           [5, 5]\n",
        "│    └─weight                                                                                [6, 16, 5, 5]\n",
        "│    └─bias                                                                                  [16]\n",
        "|-ReLU                                   [4, 16, 10, 10]           [4, 16, 10, 10]\n",
        "├─MaxPool2d: 1-4                         [4, 16, 10, 10]           [4, 16, 5, 5]             2\n",
        "├─Linear: 1-5                            [4, 400]                  [4, 120]                  --\n",
        "│    └─weight                                                                                [400, 120]\n",
        "│    └─bias                                                                                  [120]\n",
        "|-ReLU                                   [4, 120]                  [4, 120]\n",
        "├─Linear: 1-6                            [4, 120]                  [4, 84]                   --\n",
        "│    └─weight                                                                                [120, 84]\n",
        "│    └─bias                                                                                  [84]\n",
        "|-ReLU                                   [4, 84]                   [4, 84]\n",
        "├─Linear: 1-7                            [4, 84]                   [4, 10]                   --\n",
        "│    └─weight                                                                                [84, 10]\n",
        "│    └─bias                                                                                  [10]\n",
        "===================================================================================================================\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrz4ng3DRTgv"
      },
      "source": [
        "## 損失関数とオプティマイザの定義\n",
        "\n",
        "損失関数$L$とはモデルの予測値と正解ラベルの誤差を計算するための関数です。\\\n",
        "損失関数は`CrossEntropyLoss`というラベルデータの確率分布と出力の確率分布の誤差を計算する関数が主に使用されます。\\\n",
        "また、オプティマイザーとはパラメータを更新するための関数で、画像認識では`SGD`や`Adam`などが広く使用されています。\n",
        "\n",
        "今回の演習では、`SGD`をオプティマイザとして使用します。\n",
        "`SGD`の第一引数は更新するパラメータを指定しています。\\\n",
        "第二引数は学習率と呼ばれ、一度に更新するパラメータの大きさを調整しています。\\\n",
        "`momentum`や、今回は使われていませんが`weight_decay`といった引数はニューラルネットワークの学習を安定させたり過学習を抑制するために使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jYCMEESRTgv"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP7VivpURTgw"
      },
      "source": [
        "### ニューラルネットワークの学習\n",
        "\n",
        "では、先ほど定義した損失関数とoptimizerを使用して実際にニューラルネットワークの学習を行いましょう。\\\n",
        "最初に説明した通り、ニューラルネットワークの学習はミニバッチ学習と呼ばれるデータセットの一部のデータを使用してパラメータを更新する作業を繰り返し行います。\\\n",
        "データセット内の全てのデータが学習に使用されるイテレーションの数を1エポックと呼び、複数エポック学習することでパラメータの最適化を行います。\\\n",
        "\n",
        "各イテレーションでは、次の4つの処理を主に行います。\n",
        "- モデルの推論(順伝播)\n",
        "    - ミニバッチ入力に対してモデルが予測値を出力します。\n",
        "- 損失の計算\n",
        "    - モデルの予測値と正解ラベルの誤差を計算します。\n",
        "- 勾配のリセット\n",
        "    - 前のイテレーションでパラメータの更新に使用した勾配のリセット。\n",
        "- 勾配の計算(逆伝播)\n",
        "    - 損失を元に各パラメータの勾配計算を行います。\n",
        "- パラメータの更新"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKxYZSUmRTgw"
      },
      "outputs": [],
      "source": [
        "for epoch in range(2):\n",
        "    running_loss = 0.\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # 1. forward\n",
        "        outputs = net(inputs)\n",
        "        # 2. compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 3. reset parameter gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsKYKYPxRTgw"
      },
      "source": [
        "### テストデータを使用した検証\n",
        "では、先ほど学習したモデルを使用して新しい検証用データに対する精度を測ってみましょう。\n",
        "\n",
        "まずは、評価に使うデータをテストデータセットからロードします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE1RaHA9RTgx"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(images, labels)\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrUDUeABRTgx"
      },
      "source": [
        "先ほどロードしたデータを用いて推論を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8pkS2i-RTgx"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad(): # Gradient計算を無効にします\n",
        "    outputs = net(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTZ_9RE9RTgy"
      },
      "source": [
        "予測結果を出力してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DKuysY8RTgy"
      },
      "outputs": [],
      "source": [
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4L_bYqVRTgy"
      },
      "source": [
        "結果はどうでしたか。\n",
        "次は、テストデータセットを全て用いて精度を確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evBBg5LNRTgz"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHqo06ORRTgz"
      },
      "source": [
        "クラスごとの精度も確認してみましょう。\n",
        "どのクラスの精度が一番高かったですか。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2vZ-5VkRTgz"
      },
      "outputs": [],
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od6Uhnk5RTg0"
      },
      "source": [
        "学習したモデルは、次のように保存することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aegUqzT2RTg0"
      },
      "outputs": [],
      "source": [
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpA019SxRTg0"
      },
      "source": [
        "保存したモデルは次のようにすることで、保存したパラメータを使用することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMR-c2suRTg_"
      },
      "outputs": [],
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMZVErjzRThA"
      },
      "source": [
        "最後に学習をGPUを用いて行うように変更します。\\\n",
        "GPUを使用して学習するためには、モデルとデータをGPUに送る必要があります。\n",
        "\n",
        "まずは、GPUが使用可能か確認します。\n",
        "次のコードで`False`という結果が出たらGoogle Colaboratoryのランタイムを`GPU`に変更してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STjcdL0URThA"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJU910VORThA"
      },
      "source": [
        "GPUが使用可能であることがわかったら、モデルとデータをGPUで利用できるようにします。\n",
        "まず、deviceという変数を一番初めのpythonセルに追加してください。\n",
        "\n",
        "```python\n",
        "## Pytorch関連ライブラリ\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = 'cuda:0' # 0番目のGPUをdeviceとして指定\n",
        "```\n",
        "\n",
        "モデルの定義部分と、入力及びラベルデータを次のように変更してください。\n",
        "\n",
        "```python\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "```\n",
        "\n",
        "```python\n",
        "# inputs, labels = data\n",
        "inputs, labels = data[0].to(device), data[1].to(device)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oaf0Y3ESRThB"
      },
      "source": [
        "## 宿題\n",
        "**課題1**: VGG11を実装して評価してください。\\\n",
        "(**課題2**): データ拡張、モデルなどを変更してCIFAR-10の精度を向上させてください。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. データセットの作成 (y = ax + b)\n",
        "a = 2  # 傾き\n",
        "b = 3  # 切片\n",
        "x_train = np.linspace(0, 10, 100)  # 0から10までの100個のデータ点を生成\n",
        "y_train = a * x_train + b  # 真の線形関数\n",
        "\n",
        "# Tensorに変換\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32).view(-1, 1)  # (100, 1) の形に\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # (100, 1) の形に\n",
        "\n",
        "# 2. ニューラルネットワークの定義\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # 入力1、出力1の線形層\n",
        "        init.constant_(self.linear.weight, 0)  # 重みをすべて0に初期化\n",
        "        init.constant_(self.linear.bias, 0)    # バイアスをすべて0に初期化\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# モデルのインスタンスを作成\n",
        "model = LinearRegressionModel()\n",
        "\n",
        "# 3. 損失関数とオプティマイザの定義\n",
        "criterion = nn.MSELoss()  # 平均二乗誤差を損失関数として使用\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# 4. モデルのトレーニング\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    # モデルの予測\n",
        "    y_pred = model(x_train)\n",
        "\n",
        "    # 損失の計算\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # 勾配の初期化\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # バックプロパゲーション\n",
        "    loss.backward()\n",
        "\n",
        "    # 重みの更新\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100エポックごとに損失を表示\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 5. 結果の可視化\n",
        "# 訓練済みモデルによる予測\n",
        "with torch.no_grad():\n",
        "    predicted = model(x_train).detach().numpy()\n",
        "\n",
        "# 元のデータとモデルの予測をプロット\n",
        "plt.plot(x_train.numpy(), y_train.numpy(), label='Original Data', color='blue')\n",
        "plt.plot(x_train.numpy(), predicted, label='Fitted Line', color='red')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_DpezGxdTK5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nfMe7WmyTLUR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "linearresnet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}